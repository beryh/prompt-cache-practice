{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c3797b",
   "metadata": {},
   "source": [
    "# Prompt Cache Lab - Multiturn Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2d59a",
   "metadata": {},
   "source": [
    "![graph](./output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3912da",
   "metadata": {},
   "source": [
    "# 사전 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0421eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install ipywidgets\n",
    "%pip install boto3 botocore --upgrade\n",
    "%pip install pandas\n",
    "%pip install matplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "retry_config = botocore.config.Config(\n",
    "    retries={\"max_attempts\": 1, \"mode\": \"standard\"}\n",
    ")\n",
    "session = boto3.Session(\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "bedrock_client = session.client(\"bedrock-runtime\", config=retry_config)\n",
    "\n",
    "print (\"\\n== FM lists ==\")\n",
    "model_list = session.client(\"bedrock\").list_foundation_models()['modelSummaries']\n",
    "print('\\n'.join([model['modelId'] for model in model_list if model['modelId'].startswith('anth')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ecdac",
   "metadata": {},
   "source": [
    "### Multi-turn 채팅에서 Prompt Cache 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7846f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "class ConversationManager:\n",
    "    def __init__(self, model_id, document):\n",
    "        # System Prompt 정의\n",
    "        self._system_prompt = [\n",
    "            {\n",
    "                \"text\": \"주어진 문서의 내용을 바탕으로 답변을 합니다..\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": f\"## document:\\n{document} \"\n",
    "            },\n",
    "            {\n",
    "                \"cachePoint\": {\n",
    "                    \"type\": \"default\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        self._model_id = model_id\n",
    "        self._history = []\n",
    "        self._usage = []\n",
    "        \n",
    "    def query(self, query):\n",
    "        self._history.append({\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'text': query\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        response = self._chat()\n",
    "        self._history.append(response)\n",
    "    \n",
    "    def get_converstaion(self):\n",
    "        conversation = []\n",
    "        for message in self._history:\n",
    "            if message.get('role') == 'user':\n",
    "                conversation.append(\"User: \" + message.get('content')[0].get('text'))\n",
    "            else:\n",
    "                conversation.append(\"Bot: \" + message.get('content')[0].get('text'))\n",
    "        return conversation\n",
    "    \n",
    "    def get_usage(self):\n",
    "        df = pd.DataFrame(self._usage)\n",
    "        df.columns = [ \"LatencyMs\", \"CacheRead\", \"CacheWrite\", \"Inputs\", \"Outputs\"]\n",
    "        return df\n",
    "        \n",
    "    def _chat(self):\n",
    "        count = 2\n",
    "        message_list = copy.deepcopy(self._history)\n",
    "        for message in reversed(message_list):\n",
    "            if count == 0:\n",
    "                break\n",
    "            if message.get('role') == 'user':\n",
    "                message.get('content').append({\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                })\n",
    "                count = count - 1\n",
    "        \n",
    "        print(message_list)\n",
    "\n",
    "        # 추론에 필요한 Hyperparameter 정의\n",
    "        inference_config = {\n",
    "            'maxTokens': 4096,\n",
    "            'temperature': 0,\n",
    "            'topP': 1\n",
    "        }\n",
    "\n",
    "        # Converse API 호출\n",
    "        response = bedrock_client.converse(\n",
    "            system=self._system_prompt,\n",
    "            messages=message_list,\n",
    "            modelId=self._model_id,\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        \n",
    "        print(response['usage'])\n",
    "\n",
    "        self._usage.append((response['metrics']['latencyMs'], response['usage'].get('cacheReadInputTokens', 0), response['usage'].get('cacheWriteInputTokens', 0), response['usage']['inputTokens'], response['usage']['outputTokens']))\n",
    "        return response['output']['message']\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents/prompt_caching_article.md', 'r', encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"이 글의 전체 내용을 요약해주세요.\",\n",
    "    \"본문의 기술을 활용하여 해결할 수 있는 과제는 무엇인가요?\",\n",
    "    \"본문을 이해하기 위해 필요한 배경 지식은 무엇인가요?\",\n",
    "    \"이 배경지식을 갖추었는지 확인하기 위한 질문을 만들어주세요.\",\n",
    "    \"전체 내용을 이해하였는지 확인할 수 있는 질문을 다섯 개 만들어주세요.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "\n",
    "conversation = ConversationManager(model_id=model_id, document=document)\n",
    "for q in questions:\n",
    "    conversation.query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join(conversation.get_converstaion()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.get_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = conversation.get_usage()\n",
    "columns_to_stack = [\"CacheRead\", \"CacheWrite\", \"Inputs\", \"Outputs\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Create figure with two y-axes\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot stacked bars on first y-axis\n",
    "bottom_stack = [0] * len(df)\n",
    "for column in columns_to_stack:\n",
    "    ax1.bar(df.index, df[column], bottom=bottom_stack, label=column)\n",
    "    bottom_stack = [sum(x) for x in zip(bottom_stack, df[column])]\n",
    "\n",
    "# Plot line on second y-axis  \n",
    "ax2.plot(df.index, df['LatencyMs'], color='red', marker='o', label='Latency')\n",
    "\n",
    "# Set labels and title\n",
    "ax1.set_xlabel(\"Question Turn\")\n",
    "ax1.set_ylabel(\"Token Usage\")\n",
    "ax2.set_ylabel(\"Latency\")\n",
    "plt.title(\"Cache Read/Write and Latency\")\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='lower right')\n",
    "\n",
    "plt.xticks(df.index, labels=(df.index + 1), rotation=0)\n",
    "plt.ylim(0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
